{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3ce8d2-f761-41d5-9bf5-5e2c17ff61ff",
   "metadata": {},
   "source": [
    "# üè† Sarajevo Flats Scraper\n",
    "This notebook demonstrates how to collect real estate data (flats in Sarajevo Canton) from **OLX.ba**, a popular Bosnian classifieds platform.\n",
    "\n",
    "The goal is to:\n",
    "- Collect key property details (title, price, size, location, condition‚Ä¶)\n",
    "- Store them in a structured dataset (`sarajevo_flats.csv`)\n",
    "- Prepare the dataset for future analysis or machine learning (e.g. AI price estimation)\n",
    "\n",
    "We'll use **Selenium** for dynamic page loading and **BeautifulSoup** for parsing HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ac424d-6cd6-444d-bd2c-a9fcc9b7db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5716a2b2-8080-40e7-bdb1-c1cb6e4e6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firefox + Geckodriver setup\n",
    "firefox_binary = \"/usr/bin/firefox\"\n",
    "geckodriver_binary = \"/home/mustafasinanovic/miniforge3/bin/geckodriver\"\n",
    "\n",
    "# Scraper settings\n",
    "BASE_URL = \"https://olx.ba/pretraga?attr=&attr_encoded=1&q=stanovi&category_id=23&page={}&canton=9\"\n",
    "OUTPUT_CSV = \"data/sarajevo_flats.csv\"\n",
    "MAX_PAGES = 50\n",
    "REQUEST_DELAY = (2, 5)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4d7a2-3106-4c0e-b9d3-5daf7664f4c6",
   "metadata": {},
   "source": [
    "The scraper will fetch up to 50 pages of listings from the OLX search results for *Sarajevo Canton flats*.  \n",
    "All results are stored in `data/sarajevo_flats.csv`.  \n",
    "We use randomized delays between requests to reduce the risk of blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea42597d-6329-435e-b768-671f5ecefcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    return \" \".join(s.split()).strip() if s else None\n",
    "\n",
    "def extract_price(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    cleaned = re.sub(r\"[^0-9]\", \"\", text)\n",
    "    return int(cleaned) if cleaned else None\n",
    "\n",
    "def extract_number(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)\", text)\n",
    "    return int(m.group(1)) if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5027e-6315-4d68-ad90-5094d3f8bda6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ `clean_text(s)`\n",
    "\n",
    "```python\n",
    "def clean_text(s):\n",
    "    return \" \".join(s.split()).strip() if s else None\n",
    "````\n",
    "\n",
    "**Purpose:**\n",
    "Cleans messy text by removing extra spaces and newlines.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "* Splits the string into words using `.split()` (automatically handles multiple spaces).\n",
    "* Joins them back into a single string with a single space between each word.\n",
    "* Strips leading and trailing spaces.\n",
    "* Returns `None` if the input is empty.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "clean_text(\"  Spacious   apartment   in   city center  \")\n",
    "# Output: 'Spacious apartment in city center'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ `extract_price(text)`\n",
    "\n",
    "```python\n",
    "def extract_price(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    cleaned = re.sub(r\"[^0-9]\", \"\", text)\n",
    "    return int(cleaned) if cleaned else None\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "Extracts numeric price values from strings (e.g. ‚Äú250,000 KM‚Äù ‚Üí 250000).\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "* Removes all characters except digits using a regular expression (`[^0-9]`).\n",
    "* Converts the remaining digits to an integer.\n",
    "* Returns `None` if there‚Äôs no number found.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "extract_price(\"Cijena: 150,000 KM\")\n",
    "# Output: 150000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ `extract_number(text)`\n",
    "\n",
    "```python\n",
    "def extract_number(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)\", text)\n",
    "    return int(m.group(1)) if m else None\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "Extracts the **first numeric value** from a string (useful for things like area, number of rooms, etc.).\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "* Uses `re.search()` to find the first sequence of digits in the text.\n",
    "* Converts it to an integer and returns it.\n",
    "* Returns `None` if no digits are found.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "extract_number(\"Stan ima 3 sobe i 2 kupatila\")\n",
    "# Output: 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a2125e-e472-4cae-a046-5b90ae4c492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_source(url, driver):\n",
    "    \"\"\"Loads a given URL and returns its HTML source, with timeout handling.\"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        return driver.page_source\n",
    "    except (TimeoutException, WebDriverException, OSError) as e:\n",
    "        print(f\"[!] Failed to load page: {url} ‚Üí {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Unexpected error loading page: {url} ‚Üí {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3633b84-b6d0-4995-980b-5298e23c3c09",
   "metadata": {},
   "source": [
    "This function uses Selenium to load pages dynamically.\n",
    "If a page fails (timeout, network error, etc.), we log the issue but continue scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37189912-ba57-4816-bc2b-6adb24bee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_detail_page(url, driver):\n",
    "    html = fetch_page_source(url, driver)\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        get_text = lambda sel: clean_text(soup.select_one(sel).get_text()) if soup.select_one(sel) else None\n",
    "\n",
    "        title = get_text(\"h1\") or get_text(\".main-title-listing\")\n",
    "        price_numeric = extract_price(get_text(\".price-heading\"))\n",
    "\n",
    "        municipality_tag = soup.find(\"div\", class_=\"btn-pill city\")\n",
    "        if municipality_tag:\n",
    "            for svg in municipality_tag.find_all(\"svg\"):\n",
    "                svg.decompose()\n",
    "            municipality = clean_text(municipality_tag.get_text())\n",
    "        else:\n",
    "            municipality = None\n",
    "\n",
    "        rooms = extract_number(get_text(\"div.required-wrap:nth-child(5) > div:nth-child(2) > h4:nth-child(2)\"))\n",
    "        square_m2_text = get_text(\"div.required-wrap:nth-child(6) > div:nth-child(2) > h4:nth-child(2)\")\n",
    "        try:\n",
    "            square_m2 = float(square_m2_text.replace(\",\", \".\")) if square_m2_text else None\n",
    "        except:\n",
    "            square_m2 = None\n",
    "\n",
    "        details = {\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"price_numeric\": price_numeric,\n",
    "            \"municipality\": municipality,\n",
    "            \"condition\": get_text(\"div.required-wrap:nth-child(2) > div:nth-child(2) > h4:nth-child(2)\"),\n",
    "            \"ad_type\": get_text(\"div.required-wrap:nth-child(3) > div:nth-child(2) > h4:nth-child(2)\"),\n",
    "            \"property_type\": get_text(\"div.required-wrap:nth-child(4) > div:nth-child(2) > h4:nth-child(2)\"),\n",
    "            \"rooms\": rooms,\n",
    "            \"square_m2\": square_m2,\n",
    "            \"equipment\": get_text(\"div.required-wrap:nth-child(7) > div:nth-child(2) > h4:nth-child(2)\"),\n",
    "            \"level\": get_text(\"div.required-wrap:nth-child(8) > div:nth-child(2) > h4:nth-child(2)\"),\n",
    "            \"heating\": get_text(\"div.required-wrap:nth-child(9) > div:nth-child(2) > h4:nth-child(2)\")\n",
    "        }\n",
    "\n",
    "        print(\"Parsed:\", details)\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to parse details for {url} ‚Üí {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc22ac-cdd5-432e-850f-f6addfdb9e6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üè† `parse_detail_page(url, driver)` ‚Äî Parsing Flat Details from Listing Pages\n",
    "\n",
    "This function extracts detailed information about a **single flat listing** (e.g. on OLX or similar platforms) using Selenium and BeautifulSoup.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Step-by-Step Explanation\n",
    "\n",
    "```python\n",
    "def parse_detail_page(url, driver):\n",
    "    html = fetch_page_source(url, driver)\n",
    "    if not html:\n",
    "        return None\n",
    "```\n",
    "\n",
    "* Loads the HTML content of a property listing using a Selenium driver.\n",
    "* If the page cannot be loaded, returns `None`.\n",
    "\n",
    "\n",
    "```python\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "get_text = lambda sel: clean_text(soup.select_one(sel).get_text()) if soup.select_one(sel) else None\n",
    "```\n",
    "\n",
    "* Parses the HTML with **BeautifulSoup** using the fast `lxml` parser.\n",
    "* Defines a helper lambda `get_text()` to easily extract and clean text from a given CSS selector.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "title = get_text(\"h1\") or get_text(\".main-title-listing\")\n",
    "price_numeric = extract_price(get_text(\".price-heading\"))\n",
    "```\n",
    "\n",
    "* Extracts the **listing title** and **price** (converted into an integer) from the page.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "municipality_tag = soup.find(\"div\", class_=\"btn-pill city\")\n",
    "...\n",
    "municipality = clean_text(municipality_tag.get_text())\n",
    "```\n",
    "\n",
    "* Finds the **municipality** (city area) by looking for the `div` with class `btn-pill city`.\n",
    "* Removes any `<svg>` icons inside it to get clean text.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "rooms = extract_number(get_text(\"div.required-wrap:nth-child(5) > div:nth-child(2) > h4:nth-child(2)\"))\n",
    "square_m2_text = get_text(\"div.required-wrap:nth-child(6) > div:nth-child(2) > h4:nth-child(2)\")\n",
    "```\n",
    "\n",
    "* Extracts the **number of rooms** and **apartment size in m¬≤**.\n",
    "* Converts the size from a text format (e.g. `\"65,5\"`) into a float.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßæ The Final `details` Dictionary\n",
    "\n",
    "All collected attributes are stored in a dictionary:\n",
    "\n",
    "```python\n",
    "details = {\n",
    "    \"title\": title,\n",
    "    \"url\": url,\n",
    "    \"price_numeric\": price_numeric,\n",
    "    \"municipality\": municipality,\n",
    "    \"condition\": ...,\n",
    "    \"ad_type\": ...,\n",
    "    \"property_type\": ...,\n",
    "    \"rooms\": rooms,\n",
    "    \"square_m2\": square_m2,\n",
    "    \"equipment\": ...,\n",
    "    \"level\": ...,\n",
    "    \"heating\": ...\n",
    "}\n",
    "```\n",
    "\n",
    "Each field corresponds to a key property of the apartment listing ‚Äî making it easy to later convert into a Pandas DataFrame or CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Error Handling\n",
    "\n",
    "If any part of the parsing fails, the function prints an error message with the problematic URL and returns `None`:\n",
    "\n",
    "```python\n",
    "except Exception as e:\n",
    "    print(f\"[!] Failed to parse details for {url} ‚Üí {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975985c7-ef49-4641-afba-da14a9f249cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    options = Options()\n",
    "    options.binary_location = firefox_binary\n",
    "    options.add_argument(\"--headless\")\n",
    "    service = Service(executable_path=geckodriver_binary)\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    driver.set_page_load_timeout(120)\n",
    "\n",
    "    fieldnames = [\"title\",\"url\",\"price_numeric\",\"municipality\",\n",
    "                  \"condition\",\"ad_type\",\"property_type\",\"rooms\",\"square_m2\",\"equipment\",\"level\",\"heating\"]\n",
    "\n",
    "    write_header = not os.path.exists(OUTPUT_CSV)\n",
    "    with open(OUTPUT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            search_url = BASE_URL.format(page)\n",
    "            html = fetch_page_source(search_url, driver)\n",
    "            if not html:\n",
    "                print(f\"[!] Skipping search page {page}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                main_section = soup.find(\"main\", class_=\"articles\")\n",
    "                if not main_section:\n",
    "                    continue\n",
    "\n",
    "                links = [urljoin(\"https://olx.ba\", a[\"href\"]) for a in main_section.find_all(\"a\", href=True)]\n",
    "                print(f\"Page {page}: found {len(links)} listings\")\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        data = parse_detail_page(link, driver)\n",
    "                        if data:\n",
    "                            writer.writerow(data)\n",
    "                        time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "                    except Exception as e:\n",
    "                        print(f\"[!] Skipping listing {link} due to error ‚Üí {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed to parse search page {page} ‚Üí {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Finished. CSV saved at: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90fb81-9671-4ed3-a16b-b42835d66f72",
   "metadata": {},
   "source": [
    "### üï∏Ô∏è `scrape()` ‚Äî Main Scraper Function for OLX Flat Listings\n",
    "\n",
    "This is the **main control function** that automates the entire data collection process:\n",
    "it configures the browser, navigates through search pages, extracts listing details,\n",
    "and saves them into a CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Step 1: Configure the Firefox WebDriver\n",
    "\n",
    "```python\n",
    "options = Options()\n",
    "options.binary_location = firefox_binary\n",
    "options.add_argument(\"--headless\")\n",
    "service = Service(executable_path=geckodriver_binary)\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "driver.set_page_load_timeout(120)\n",
    "````\n",
    "\n",
    "* Sets up **Firefox in headless mode** (no visible browser window).\n",
    "* Uses the pre-defined paths for the Firefox binary and `geckodriver`.\n",
    "* A **page load timeout** of 120 seconds prevents the scraper from hanging indefinitely.\n",
    "\n",
    "This allows the script to load dynamic pages from OLX efficiently using Selenium.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìÑ Step 2: Prepare the CSV File\n",
    "\n",
    "```python\n",
    "fieldnames = [\"title\",\"url\",\"price_numeric\",\"municipality\",\n",
    "              \"condition\",\"ad_type\",\"property_type\",\"rooms\",\"square_m2\",\"equipment\",\"level\",\"heating\"]\n",
    "\n",
    "write_header = not os.path.exists(OUTPUT_CSV)\n",
    "with open(OUTPUT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "```\n",
    "\n",
    "* Defines the column names that match the fields extracted by `parse_detail_page()`.\n",
    "* Checks whether the output CSV already exists ‚Äî if not, it writes a **header row**.\n",
    "* Opens the CSV file in **append mode**, so the scraper can continue from previous runs without overwriting existing data.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîé Step 3: Iterate Through Search Pages\n",
    "\n",
    "```python\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    search_url = BASE_URL.format(page)\n",
    "    html = fetch_page_source(search_url, driver)\n",
    "```\n",
    "\n",
    "* Loops over multiple search result pages (e.g. page 1, 2, 3, ...).\n",
    "* Dynamically inserts the page number into the base URL template.\n",
    "* Uses `fetch_page_source()` to load each page and get its HTML.\n",
    "\n",
    "If the page fails to load, it prints a warning and skips to the next one.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Step 4: Extract All Listing Links\n",
    "\n",
    "```python\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "main_section = soup.find(\"main\", class_=\"articles\")\n",
    "links = [urljoin(\"https://olx.ba\", a[\"href\"]) for a in main_section.find_all(\"a\", href=True)]\n",
    "```\n",
    "\n",
    "* Parses the page using BeautifulSoup to locate the main listings section.\n",
    "* Collects all the **individual listing URLs**.\n",
    "* Prepends the base domain (`https://olx.ba`) to each relative link to get full URLs.\n",
    "\n",
    "---\n",
    "\n",
    "#### üè† Step 5: Parse Each Listing and Save the Data\n",
    "\n",
    "```python\n",
    "for link in links:\n",
    "    data = parse_detail_page(link, driver)\n",
    "    if data:\n",
    "        writer.writerow(data)\n",
    "    time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "```\n",
    "\n",
    "* Visits each listing one by one.\n",
    "* Calls the previously defined `parse_detail_page()` to extract detailed information.\n",
    "* If valid data is returned, writes it as a new row in the CSV file.\n",
    "* Adds a **random delay** between requests to avoid being blocked by the site.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßØ Step 6: Handle Errors Gracefully\n",
    "\n",
    "If any listing or page causes an error (timeout, missing element, etc.),\n",
    "the function catches the exception, logs it, and **continues scraping** instead of stopping.\n",
    "\n",
    "```python\n",
    "except Exception as e:\n",
    "    print(f\"[!] Skipping listing {link} due to error ‚Üí {e}\")\n",
    "```\n",
    "\n",
    "This ensures that a few broken pages don‚Äôt interrupt the entire process.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Step 7: Close the Browser and Finish\n",
    "\n",
    "```python\n",
    "driver.quit()\n",
    "print(f\"Finished. CSV saved at: {OUTPUT_CSV}\")\n",
    "```\n",
    "\n",
    "* After all pages are processed, the Selenium WebDriver is closed.\n",
    "* A success message confirms the output file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3276cb29-a9ad-41a4-94b6-5f54e8c718dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mscrape\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, MAX_PAGES + \u001b[32m1\u001b[39m):\n\u001b[32m     19\u001b[39m     search_url = BASE_URL.format(page)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     html = \u001b[43mfetch_page_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m html:\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[!] Skipping search page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mfetch_page_source\u001b[39m\u001b[34m(url, driver)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads a given URL and returns its HTML source, with timeout handling.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     time.sleep(\u001b[32m3\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m driver.page_source\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:483\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:455\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m    453\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m response = \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRemoteConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mself\u001b[39m.error_handler.check_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:407\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    405\u001b[39m trimmed = \u001b[38;5;28mself\u001b[39m._trim_large_entries(params)\n\u001b[32m    406\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:431\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    428\u001b[39m     body = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     statuscode = response.status\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_url(\n\u001b[32m    136\u001b[39m         method,\n\u001b[32m    137\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m         **urlopen_kw,\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43murlopen_kw\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    274\u001b[39m     extra_kw[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m].setdefault(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, content_type)\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/poolmanager.py:459\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    457\u001b[39m     response = conn.urlopen(method, url, **kw)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc5e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb29458b",
   "metadata": {},
   "source": [
    "## üìä Data Inspection - OLX Merged Dataset\n",
    "\n",
    "Let's load and inspect the scraped data from the merged OLX CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24232cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../data/sarajevo_flats_merged_olx.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89fe697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"FIRST 5 ROWS:\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39280595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types and missing values\n",
    "print(\"DATA TYPES AND MISSING VALUES:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ce53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numeric columns\n",
    "print(\"STATISTICAL SUMMARY (Numeric Columns):\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values per column\n",
    "print(\"MISSING VALUES PER COLUMN:\")\n",
    "print(\"=\" * 80)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage.round(2)\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
