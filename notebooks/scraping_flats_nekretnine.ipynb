{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3ce8d2-f761-41d5-9bf5-5e2c17ff61ff",
   "metadata": {},
   "source": [
    "# üè† Sarajevo Flats Scraper\n",
    "This notebook demonstrates how to collect real estate data (flats in Sarajevo Canton) from **NEKRETNINE.ba**, a popular Bosnian classifieds platform.\n",
    "\n",
    "The goal is to:\n",
    "- Collect key property details (title, price, size, location, condition‚Ä¶)\n",
    "- Store them in a structured dataset (`sarajevo_flats.csv`)\n",
    "- Prepare the dataset for future analysis or machine learning (e.g. AI price estimation)\n",
    "\n",
    "We'll use **Selenium** for dynamic page loading and **BeautifulSoup** for parsing HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1cbee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.12.11' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PYTHON ENVIRONMENT INFO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPython executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"\\nPython path (where packages are loaded from):\")\n",
    "for i, path in enumerate(sys.path[:5], 1):\n",
    "    print(f\"  {i}. {path}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac424d-6cd6-444d-bd2c-a9fcc9b7db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716a2b2-8080-40e7-bdb1-c1cb6e4e6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firefox + Geckodriver setup\n",
    "firefox_binary = \"/usr/bin/firefox\"\n",
    "geckodriver_binary = \"/home/mustafasinanovic/miniforge3/bin/geckodriver\"\n",
    "\n",
    "# Scraper settings\n",
    "BASE_URL = \"https://nekretnine.ba/listing.php?lang=ba&sel=nekretnine&grad=65&naselje=&kat=3&subjekt=2&cij1=&cij2=&pov1=&pov2=&spr1=&spr2=&firma=&page={}\"\n",
    "OUTPUT_CSV = \"data/sarajevo_flats_nekretnine.csv\"\n",
    "MAX_PAGES = 88\n",
    "REQUEST_DELAY = (2, 5)\n",
    "\n",
    "# Multithreading settings\n",
    "MAX_WORKERS = 3  # Number of parallel browser instances (don't set too high to avoid blocking)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4d7a2-3106-4c0e-b9d3-5daf7664f4c6",
   "metadata": {},
   "source": [
    "The scraper will fetch up to 88 pages of listings from the OLX search results for *Sarajevo Canton flats*.  \n",
    "All results are stored in `data/sarajevo_flats_nekretnine.csv`.  \n",
    "We use randomized delays between requests to reduce the risk of blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42597d-6329-435e-b768-671f5ecefcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    return \" \".join(s.split()).strip() if s else None\n",
    "\n",
    "def extract_price(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    cleaned = re.sub(r\"[^0-9]\", \"\", text)\n",
    "    return int(cleaned) if cleaned else None\n",
    "\n",
    "def extract_number(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)\", text)\n",
    "    return int(m.group(1)) if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5027e-6315-4d68-ad90-5094d3f8bda6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2125e-e472-4cae-a046-5b90ae4c492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_source(url, driver, short_wait=10):\n",
    "    \"\"\"\n",
    "    Loads a given URL and returns whatever HTML is available immediately.\n",
    "    Does NOT wait for the page to fully load (useful for slow or problematic websites).\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL to load\n",
    "        driver (webdriver): Selenium WebDriver instance\n",
    "        short_wait (int or float): seconds to wait after opening page before returning source\n",
    "\n",
    "    Returns:\n",
    "        str or None: HTML source (may be partially loaded)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[+] Attempting to load URL quickly: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(short_wait)  # minimal wait to let some content render\n",
    "        html = driver.page_source\n",
    "        if html:\n",
    "            print(f\"[+] HTML fetched (may be partial): {url}\")\n",
    "        else:\n",
    "            print(f\"[!] No HTML returned for {url}\")\n",
    "        return html\n",
    "    except (TimeoutException, WebDriverException, OSError) as e:\n",
    "        print(f\"[!] Failed to load page: {url} ‚Üí {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Unexpected error loading page: {url} ‚Üí {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3633b84-b6d0-4995-980b-5298e23c3c09",
   "metadata": {},
   "source": [
    "This function uses Selenium to load pages dynamically.\n",
    "If a page fails (timeout, network error, etc.), we log the issue but continue scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37189912-ba57-4816-bc2b-6adb24bee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_detail_page(url, driver):\n",
    "    html = fetch_page_source(url, driver)\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # Extract title\n",
    "        title_elem = soup.select_one(\"div.listing-titlebar-title h2\")\n",
    "        if title_elem:\n",
    "            # Remove the tag span from title\n",
    "            tag_span = title_elem.find(\"span\", class_=\"listing-tag\")\n",
    "            if tag_span:\n",
    "                tag_span.decompose()\n",
    "            title = clean_text(title_elem.get_text())\n",
    "        else:\n",
    "            title = None\n",
    "\n",
    "        # Extract municipality (address/location)\n",
    "        municipality_elem = soup.select_one(\"a.listing-address\")\n",
    "        municipality = clean_text(municipality_elem.get_text()) if municipality_elem else None\n",
    "\n",
    "        # Extract price\n",
    "        price_elem = soup.select_one(\"span.re-slidep\")\n",
    "        price_numeric = extract_price(price_elem.get_text()) if price_elem else None\n",
    "\n",
    "        # Extract property type\n",
    "        property_type_elem = soup.find(\"b\", string=\"TIP\")\n",
    "        property_type = clean_text(property_type_elem.find_next(\"div\").get_text()) if property_type_elem else None\n",
    "\n",
    "        # Extract ad type (subject - prodaja/izdavanje)\n",
    "        ad_type_elem = soup.find(\"b\", string=\"SUBJEKT\")\n",
    "        ad_type = clean_text(ad_type_elem.find_next(\"div\").get_text()) if ad_type_elem else None\n",
    "\n",
    "        # Extract rooms\n",
    "        rooms_elem = soup.find(\"b\", string=\"BROJ SOBA\")\n",
    "        rooms = clean_text(rooms_elem.find_next(\"div\").get_text()) if rooms_elem else None\n",
    "\n",
    "        # Extract square meters\n",
    "        square_m2_elem = soup.find(\"b\", string=\"POVR≈†INA\")\n",
    "        if square_m2_elem:\n",
    "            area_text = square_m2_elem.find_next(\"div\").get_text(strip=True)\n",
    "            # Extract number and convert to float\n",
    "            area_match = re.search(r'([\\d,\\.]+)', area_text)\n",
    "            if area_match:\n",
    "                area_str = area_match.group(1).replace(',', '.')\n",
    "                try:\n",
    "                    square_m2 = float(area_str)\n",
    "                except:\n",
    "                    square_m2 = None\n",
    "            else:\n",
    "                square_m2 = None\n",
    "        else:\n",
    "            square_m2 = None\n",
    "\n",
    "        # Extract description\n",
    "        description_head = soup.find(\"h3\", string=re.compile(\"Opis nekretnine\"))\n",
    "        description = clean_text(description_head.find_next(\"p\").get_text(\" \")) if description_head else None\n",
    "\n",
    "        # Extract equipment/amenities\n",
    "        equipment_list = [clean_text(li.get_text()) for li in soup.select(\"ul.listing-features li\")]\n",
    "        equipment = \", \".join([e for e in equipment_list if e])  # Filter out None values\n",
    "\n",
    "        details = {\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"price_numeric\": price_numeric,\n",
    "            \"municipality\": municipality,\n",
    "            \"property_type\": property_type,\n",
    "            \"ad_type\": ad_type,\n",
    "            \"rooms\": rooms,\n",
    "            \"square_m2\": square_m2,\n",
    "            \"equipment\": equipment,\n",
    "            \"description\": description\n",
    "        }\n",
    "\n",
    "        print(\"Parsed:\", details)\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to parse details for {url} ‚Üí {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc22ac-cdd5-432e-850f-f6addfdb9e6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1b33c-19e3-4ad3-82c7-b16b2b757fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver():\n",
    "    print(\"[*] Initializing Firefox WebDriver...\")\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.binary_location = firefox_binary\n",
    "        options.add_argument(\"--headless\")\n",
    "\n",
    "        # ‚úÖ New way to set pageLoadStrategy (Selenium 4+)\n",
    "        options.set_capability(\"pageLoadStrategy\", \"none\")\n",
    "\n",
    "        service = Service(executable_path=geckodriver_binary)\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        driver.set_page_load_timeout(10)\n",
    "        print(\"[+] WebDriver started successfully.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to start Firefox driver: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b938b2",
   "metadata": {},
   "source": [
    "## Multithreaded Scraping Functions\n",
    "\n",
    "We'll use ThreadPoolExecutor to run multiple Selenium instances in parallel. Each thread gets its own WebDriver instance to avoid conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975985c7-ef49-4641-afba-da14a9f249cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing(link, driver):\n",
    "    \"\"\"\n",
    "    Scrape a single listing and return the data.\n",
    "    Each thread will call this function with its own driver instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = parse_detail_page(link, driver)\n",
    "        if data:\n",
    "            print(f\"      ‚úî Scraped: {link}\")\n",
    "        else:\n",
    "            print(f\"      ‚úñ Failed: {link}\")\n",
    "        time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error scraping {link}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_page_listings(page_num, driver):\n",
    "    \"\"\"\n",
    "    Scrape all listings from a single search results page.\n",
    "    Returns a list of listing URLs found on that page.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[+] Fetching search page {page_num}: {BASE_URL.format(page_num)}\")\n",
    "    html = fetch_page_source(BASE_URL.format(page_num), driver)\n",
    "    \n",
    "    if not html:\n",
    "        print(f\"[!] No HTML for page {page_num}, skipping.\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        links = [urljoin(\"https://nekretnine.ba/\", a[\"href\"]) \n",
    "                for a in soup.find_all(\"a\", href=re.compile(r\"^real-estate\\.php\\?lang=ba&sel=nekretnine&view=\"))]\n",
    "        \n",
    "        print(f\"  ‚Üí Found {len(links)} listings on page {page_num}\")\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"[!] No links found on page {page_num}. Possible structure change?\")\n",
    "        \n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to parse search page {page_num} ‚Üí {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_with_threading():\n",
    "    \"\"\"\n",
    "    Multithreaded scraping function.\n",
    "    Creates multiple WebDriver instances and processes listings in parallel.\n",
    "    \"\"\"\n",
    "    fieldnames = [\"title\", \"url\", \"price_numeric\", \"municipality\", \"property_type\", \"ad_type\", \"rooms\", \"square_m2\", \"equipment\", \"description\"]\n",
    "    \n",
    "    # Thread-safe lock for writing to CSV\n",
    "    csv_lock = threading.Lock()\n",
    "    \n",
    "    # Create main driver for collecting listing URLs\n",
    "    print(\"[*] Creating main driver for collecting listing URLs...\")\n",
    "    main_driver = create_driver()\n",
    "    if not main_driver:\n",
    "        print(\"[!] Failed to create main driver. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Collect all listing URLs first\n",
    "    print(f\"[*] Collecting listing URLs from {MAX_PAGES} pages...\")\n",
    "    all_listing_urls = []\n",
    "    \n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        links = scrape_page_listings(page, main_driver)\n",
    "        all_listing_urls.extend(links)\n",
    "        time.sleep(random.uniform(1, 2))  # Small delay between pages\n",
    "    \n",
    "    main_driver.quit()\n",
    "    print(f\"\\n[+] Collected {len(all_listing_urls)} total listings to scrape.\")\n",
    "    \n",
    "    if not all_listing_urls:\n",
    "        print(\"[!] No listings found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare CSV file\n",
    "    write_header = not os.path.exists(OUTPUT_CSV)\n",
    "    \n",
    "    def worker_scrape(url_batch):\n",
    "        \"\"\"Worker function that each thread will execute\"\"\"\n",
    "        driver = create_driver()\n",
    "        if not driver:\n",
    "            print(\"[!] Failed to create worker driver\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for url in url_batch:\n",
    "            data = scrape_listing(url, driver)\n",
    "            if data:\n",
    "                results.append(data)\n",
    "        \n",
    "        driver.quit()\n",
    "        return results\n",
    "    \n",
    "    # Split listings into batches for each worker\n",
    "    batch_size = len(all_listing_urls) // MAX_WORKERS + 1\n",
    "    url_batches = [all_listing_urls[i:i + batch_size] for i in range(0, len(all_listing_urls), batch_size)]\n",
    "    \n",
    "    print(f\"\\n[*] Starting multithreaded scraping with {MAX_WORKERS} workers...\")\n",
    "    print(f\"[*] Processing {len(url_batches)} batches...\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel scraping\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all batches to thread pool\n",
    "        futures = {executor.submit(worker_scrape, batch): i for i, batch in enumerate(url_batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            batch_num = futures[future]\n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "                print(f\"[+] Batch {batch_num + 1}/{len(url_batches)} completed. Scraped {len(batch_results)} listings.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Batch {batch_num + 1} failed: {e}\")\n",
    "    \n",
    "    # Write all results to CSV at once (thread-safe)\n",
    "    print(f\"\\n[*] Writing {len(all_results)} results to CSV...\")\n",
    "    with csv_lock:\n",
    "        with open(OUTPUT_CSV, \"a\" if not write_header else \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "                print(f\"[+] Created new CSV file: {OUTPUT_CSV}\")\n",
    "            \n",
    "            for data in all_results:\n",
    "                writer.writerow(data)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Finished scraping. Data saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"‚úÖ Total listings scraped: {len(all_results)}/{len(all_listing_urls)}\")\n",
    "\n",
    "\n",
    "def scrape():\n",
    "    \"\"\"Original single-threaded scraping function (kept for reference)\"\"\"\n",
    "    driver = create_driver()\n",
    "\n",
    "    fieldnames = [\"title\", \"url\", \"price_numeric\", \"municipality\", \"property_type\", \"ad_type\", \"rooms\", \"square_m2\", \"equipment\", \"description\"]\n",
    "\n",
    "    write_header = not os.path.exists(OUTPUT_CSV)\n",
    "    with open(OUTPUT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "            print(f\"[+] Created new CSV file: {OUTPUT_CSV}\")\n",
    "        else:\n",
    "            print(f\"[+] Appending to existing CSV: {OUTPUT_CSV}\")\n",
    "\n",
    "        print(f\"[*] Starting scraping of up to {MAX_PAGES} pages...\")\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            print(f\"\\n[+] Fetching search page {page}: {BASE_URL.format(page)}\")\n",
    "            html = fetch_page_source(BASE_URL.format(page), driver)\n",
    "            if not html:\n",
    "                print(f\"[!] No HTML for page {page}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "                links = [urljoin(\"https://nekretnine.ba/\", a[\"href\"]) for a in soup.find_all(\"a\", href=re.compile(r\"^real-estate\\.php\\?lang=ba&sel=nekretnine&view=\"))]\n",
    "                \n",
    "                print(f\"  ‚Üí Found {len(links)} listings on page {page}\")\n",
    "\n",
    "                if not links:\n",
    "                    print(f\"[!] No links found on page {page}. Possible structure change?\")\n",
    "                    continue\n",
    "\n",
    "                for i, link in enumerate(links, start=1):\n",
    "                    print(f\"    [{i}/{len(links)}] Scraping listing: {link}\")\n",
    "                    try:\n",
    "                        data = parse_detail_page(link, driver)\n",
    "                        if data:\n",
    "                            writer.writerow(data)\n",
    "                            print(\"      ‚úî Saved listing data to CSV.\")\n",
    "                        else:\n",
    "                            print(\"      ‚úñ No data parsed, skipping.\")\n",
    "                        time.sleep(random.uniform(*REQUEST_DELAY))\n",
    "                    except Exception as e:\n",
    "                        print(f\"[!] Error scraping {link}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed to parse search page {page} ‚Üí {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\n‚úÖ Finished scraping. Data saved to: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90fb81-9671-4ed3-a16b-b42835d66f72",
   "metadata": {},
   "source": [
    "## Run the Scraper\n",
    "\n",
    "Choose which scraper to run:\n",
    "- `scrape_with_threading()` - **Multithreaded version** (faster, uses 3 parallel browsers)\n",
    "- `scrape()` - Single-threaded version (slower, but more stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276cb29-a9ad-41a4-94b6-5f54e8c718dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Use multithreaded version for faster scraping\n",
    "#     scrape_with_threading()\n",
    "    \n",
    "    # Or use single-threaded version (comment above, uncomment below)\n",
    "    # scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6defac02",
   "metadata": {},
   "source": [
    "## üìä Data Inspection\n",
    "\n",
    "Let's load and inspect the scraped data from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../data/sarajevo_flats_nekretnine_cleaned.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e12559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"FIRST 5 ROWS:\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520727eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types and missing values\n",
    "print(\"DATA TYPES AND MISSING VALUES:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numeric columns\n",
    "print(\"STATISTICAL SUMMARY (Numeric Columns):\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aca162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values per column\n",
    "print(\"MISSING VALUES PER COLUMN:\")\n",
    "print(\"=\" * 80)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage.round(2)\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price analysis\n",
    "print(\"PRICE ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Average Price: {df['price_numeric'].mean():.2f} KM\")\n",
    "print(f\"Median Price: {df['price_numeric'].median():.2f} KM\")\n",
    "print(f\"Min Price: {df['price_numeric'].min():.2f} KM\")\n",
    "print(f\"Max Price: {df['price_numeric'].max():.2f} KM\")\n",
    "print(f\"Standard Deviation: {df['price_numeric'].std():.2f} KM\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square meters analysis\n",
    "print(\"SQUARE METERS ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Average Area: {df['square_m2'].mean():.2f} m¬≤\")\n",
    "print(f\"Median Area: {df['square_m2'].median():.2f} m¬≤\")\n",
    "print(f\"Min Area: {df['square_m2'].min():.2f} m¬≤\")\n",
    "print(f\"Max Area: {df['square_m2'].max():.2f} m¬≤\")\n",
    "print(f\"Standard Deviation: {df['square_m2'].std():.2f} m¬≤\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce70309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unrealistic property sizes (> 300 m¬≤)\n",
    "print(\"FILTERING UNREALISTIC PROPERTY SIZES:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Records before filtering: {len(df)}\")\n",
    "\n",
    "# Show properties that will be removed\n",
    "large_properties = df[df['square_m2'] > 300]\n",
    "if len(large_properties) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(large_properties)} properties with area > 300 m¬≤:\")\n",
    "    print(large_properties[['title', 'square_m2', 'property_type', 'url']])\n",
    "    \n",
    "    # Remove properties with area > 300 m¬≤\n",
    "    df = df[df['square_m2'] <= 300]\n",
    "    print(f\"\\n‚úÖ Filtered out {len(large_properties)} properties\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No properties with area > 300 m¬≤ found\")\n",
    "\n",
    "print(f\"Records after filtering: {len(df)}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the cleaned data to a new CSV file\n",
    "print(\"SAVING CLEANED DATA:\")\n",
    "print(\"=\" * 80)\n",
    "output_file = '../data/sarajevo_flats_nekretnine_cleaned.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Cleaned data saved to: {output_file}\")\n",
    "print(f\"Total records saved: {len(df)}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66974a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price per square meter analysis (filter out invalid data first)\n",
    "print(\"PRICE PER SQUARE METER ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for zero or null square_m2 values\n",
    "print(f\"Properties with square_m2 = 0 or NaN: {((df['square_m2'] == 0) | df['square_m2'].isna()).sum()}\")\n",
    "print(f\"Properties with price_numeric = 0 or NaN: {((df['price_numeric'] == 0) | df['price_numeric'].isna()).sum()}\")\n",
    "\n",
    "# Filter out properties with invalid data for price per m¬≤ calculation\n",
    "valid_df = df[(df['square_m2'] > 0) & (df['price_numeric'] > 0) & df['square_m2'].notna() & df['price_numeric'].notna()].copy()\n",
    "\n",
    "print(f\"\\nValid properties for price/m¬≤ analysis: {len(valid_df)}/{len(df)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate price per m¬≤ only on valid data\n",
    "valid_df['price_per_m2'] = valid_df['price_numeric'] / valid_df['square_m2']\n",
    "\n",
    "print(f\"\\nAverage Price per m¬≤: {valid_df['price_per_m2'].mean():.2f} KM/m¬≤\")\n",
    "print(f\"Median Price per m¬≤: {valid_df['price_per_m2'].median():.2f} KM/m¬≤\")\n",
    "print(f\"Min Price per m¬≤: {valid_df['price_per_m2'].min():.2f} KM/m¬≤\")\n",
    "print(f\"Max Price per m¬≤: {valid_df['price_per_m2'].max():.2f} KM/m¬≤\")\n",
    "print(f\"Standard Deviation: {valid_df['price_per_m2'].std():.2f} KM/m¬≤\")\n",
    "\n",
    "# Add price_per_m2 back to main dataframe\n",
    "df['price_per_m2'] = df.apply(\n",
    "    lambda row: row['price_numeric'] / row['square_m2'] \n",
    "    if (row['square_m2'] > 0 and row['price_numeric'] > 0) \n",
    "    else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "print(\"DUPLICATE RECORDS CHECK:\")\n",
    "print(\"=\" * 80)\n",
    "duplicates = df.duplicated(subset=['url']).sum()\n",
    "print(f\"Number of duplicate URLs: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate URLs found:\")\n",
    "    print(df[df.duplicated(subset=['url'], keep=False)][['title', 'url', 'price_numeric']])\n",
    "else:\n",
    "    print(\"No duplicate URLs found!\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf35679",
   "metadata": {},
   "source": [
    "## üìã Complete Categorical Values Analysis\n",
    "\n",
    "Let's examine all possible values for each categorical column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90737874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns (object dtype or low cardinality numeric columns)\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL UNIQUE VALUES FOR CATEGORICAL COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all columns\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Separate numeric and non-numeric columns\n",
    "categorical_cols = []\n",
    "numeric_cols = []\n",
    "\n",
    "for col in all_columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        categorical_cols.append(col)\n",
    "    elif df[col].dtype in ['int64', 'float64']:\n",
    "        # Check if it's a low cardinality numeric column (might be categorical)\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count <= 20:  # Threshold for categorical numeric columns\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            numeric_cols.append(col)\n",
    "    else:\n",
    "        categorical_cols.append(col)\n",
    "\n",
    "# remove identifier columns from categorical list\n",
    "categorical_cols = [c for c in categorical_cols if c not in ('title', 'url', 'description', 'equipment')]\n",
    "print(f\"\\nFound {len(categorical_cols)} categorical columns\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"\\nNumeric columns (excluded): {numeric_cols}\")\n",
    "print(f\"\\nText columns (excluded): title, url, description, equipment\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all unique values for each categorical column\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL POSSIBLE VALUES FOR EACH CATEGORICAL COLUMN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìå COLUMN: {col.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get unique values (excluding NaN)\n",
    "    unique_values = df[col].dropna().unique()\n",
    "    unique_count = len(unique_values)\n",
    "    \n",
    "    print(f\"Total unique values: {unique_count}\")\n",
    "    print(f\"\\nAll possible values:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Sort values for better readability\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        sorted_values = sorted(unique_values)\n",
    "    else:\n",
    "        sorted_values = sorted(unique_values, key=lambda x: str(x))\n",
    "    \n",
    "    # Display all unique values in a clean list\n",
    "    for i, value in enumerate(sorted_values, 1):\n",
    "        print(f\"{i:3d}. {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100bfaf",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Municipality Standardization\n",
    "\n",
    "Let's check if addresses or neighborhoods in the data can be mapped to the 9 standard municipalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Analyze current municipality values and look for patterns\n",
    "print(\"=\" * 80)\n",
    "print(\"CURRENT MUNICIPALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'municipality' in df.columns:\n",
    "    print(f\"\\nTotal unique municipality values: {df['municipality'].nunique()}\")\n",
    "    print(f\"Missing municipality values: {df['municipality'].isnull().sum()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ALL MUNICIPALITY VALUES (with counts):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    municipality_counts = df['municipality'].value_counts()\n",
    "    for idx, (municipality, count) in enumerate(municipality_counts.items(), 1):\n",
    "        print(f\"{idx:3d}. {municipality:<50} ({count} records)\")\n",
    "    \n",
    "    # Define target municipalities\n",
    "    target_municipalities = [\n",
    "        'Had≈æiƒái',\n",
    "        'Ilid≈æa',\n",
    "        'Ilija≈°',\n",
    "        'Sarajevo - Centar',\n",
    "        'Sarajevo - Novi Grad',\n",
    "        'Sarajevo - Novo Sarajevo',\n",
    "        'Sarajevo - Stari Grad',\n",
    "        'Trnovo',\n",
    "        'Vogo≈°ƒáa'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TARGET MUNICIPALITIES (from reference):\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, municipality in enumerate(target_municipalities, 1):\n",
    "        count = (df['municipality'] == municipality).sum()\n",
    "        print(f\"{idx:3d}. {municipality:<30} ({count} records in dataset)\")\n",
    "    \n",
    "    # Check which values don't match target municipalities\n",
    "    non_matching = df[~df['municipality'].isin(target_municipalities) & df['municipality'].notna()]\n",
    "    \n",
    "    if len(non_matching) > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"‚ö†Ô∏è FOUND {len(non_matching)} RECORDS WITH NON-STANDARD MUNICIPALITY VALUES\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nThese need to be mapped to one of the 9 target municipalities:\")\n",
    "        non_matching_counts = non_matching['municipality'].value_counts()\n",
    "        for municipality, count in non_matching_counts.items():\n",
    "            print(f\"  - {municipality:<50} ({count} records)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All municipality values already match the target municipalities!\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No 'municipality' column found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a comprehensive mapping of neighborhoods to municipalities\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING NEIGHBORHOOD-TO-MUNICIPALITY MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define comprehensive mapping based on Sarajevo Canton geography\n",
    "# This maps neighborhoods, areas, and alternate names to their parent municipalities\n",
    "neighborhood_to_municipality = {\n",
    "    # Sarajevo - Centar (Central Sarajevo)\n",
    "    'Centar': 'Sarajevo - Centar',\n",
    "    'Marijin Dvor': 'Sarajevo - Centar',\n",
    "    'Skenderija': 'Sarajevo - Centar',\n",
    "    'Mejtas': 'Sarajevo - Centar',\n",
    "    'Mejta≈°': 'Sarajevo - Centar',\n",
    "    'mejta≈°': 'Sarajevo - Centar',\n",
    "    'D≈æid≈æikovac': 'Sarajevo - Centar',\n",
    "    'Bjelave': 'Sarajevo - Centar', \n",
    "    'ƒåobanija': 'Sarajevo - Centar',\n",
    "    '≈†ip': 'Sarajevo - Centar',\n",
    "    'Pearl-≈†ip': 'Sarajevo - Centar',\n",
    "    'Parl-≈†ip': 'Sarajevo - Centar',\n",
    "    'Ko≈°evo': 'Sarajevo - Centar',\n",
    "    'Ko≈°evsko brdo': 'Sarajevo - Centar',\n",
    "    'Drvenija': 'Sarajevo - Centar',\n",
    "    'Ferhadija': 'Sarajevo - Centar',\n",
    "    'Breka': 'Sarajevo - Centar',\n",
    "    'Soukbunar': 'Sarajevo - Centar',\n",
    "    \n",
    "    # Sarajevo - Stari Grad (Old Town)\n",
    "    'Stari Grad': 'Sarajevo - Stari Grad',\n",
    "    'Ba≈°ƒçar≈°ija': 'Sarajevo - Stari Grad',\n",
    "    'Alifakovac': 'Sarajevo - Stari Grad',\n",
    "    'Jekovac': 'Sarajevo - Stari Grad',\n",
    "    'Kovaƒçi': 'Sarajevo - Stari Grad',\n",
    "    'Vratnik': 'Sarajevo - Stari Grad',\n",
    "    'Sedrenik': 'Sarajevo - Stari Grad',\n",
    "    'Hrid': 'Sarajevo - Stari Grad',\n",
    "    'Bistrik': 'Sarajevo - Stari Grad',\n",
    "    \n",
    "    # Sarajevo - Novo Sarajevo (New Sarajevo)\n",
    "    'Novo Sarajevo': 'Sarajevo - Novo Sarajevo',\n",
    "    'Grbavica': 'Sarajevo - Novo Sarajevo',\n",
    "    'Dolac Malta': 'Sarajevo - Novo Sarajevo',\n",
    "    'Ciglane': 'Sarajevo - Novo Sarajevo',\n",
    "    'Hrasno': 'Sarajevo - Novo Sarajevo',\n",
    "    'Vele≈°iƒái': 'Sarajevo - Novo Sarajevo',\n",
    "    'Kovaƒçiƒái': 'Sarajevo - Novo Sarajevo',\n",
    "    'Kovacici': 'Sarajevo - Novo Sarajevo',\n",
    "    'Vraca': 'Sarajevo - Novo Sarajevo',\n",
    "    'Zmaja od Bosne': 'Sarajevo - Novo Sarajevo',\n",
    "    'Pofaliƒái': 'Sarajevo - Novo Sarajevo',\n",
    "    'Socijalno': 'Sarajevo - Novo Sarajevo',\n",
    "    'Robot Socijalno': 'Sarajevo - Novo Sarajevo',\n",
    "    'Sarajevo Tower': 'Sarajevo - Novo Sarajevo',\n",
    "    \n",
    "    # Sarajevo - Novi Grad (New City)\n",
    "    'Novi Grad': 'Sarajevo - Novi Grad',\n",
    "    '≈†vrakino Selo': 'Sarajevo - Novi Grad',\n",
    "    'Alipa≈°ino Polje': 'Sarajevo - Novi Grad',\n",
    "    'Alipa≈°ino': 'Sarajevo - Novi Grad',\n",
    "    'ƒåengiƒá Vila': 'Sarajevo - Novi Grad',\n",
    "    'Zabrƒëe': 'Sarajevo - Novi Grad',\n",
    "    'Stupsko Brdo': 'Sarajevo - Novi Grad',\n",
    "    'Buƒáa Potok': 'Sarajevo - Novi Grad',\n",
    "    'Vojniƒçko polje': 'Sarajevo - Novi Grad',\n",
    "    'Mali Bosmal': 'Sarajevo - Novi Grad',\n",
    "    'Fra Antuna Kne≈æeviƒáa': 'Sarajevo - Novi Grad',\n",
    "    'Aerodromsko naslje': 'Sarajevo - Novi Grad',\n",
    "    'Miljacka': 'Sarajevo - Novi Grad',\n",
    "    'Bulevar': 'Sarajevo - Novi Grad',\n",
    "    'Teheranski trg': 'Sarajevo - Novi Grad',\n",
    "    'Mojmilo': 'Sarajevo - Novi Grad',\n",
    "    'Dobrinja': 'Sarajevo - Novi Grad',\n",
    "    'Otoka': 'Sarajevo - Novi Grad',\n",
    "    'Aneks': 'Sarajevo - Novi Grad',\n",
    "    \n",
    "    \n",
    "    # Ilid≈æa\n",
    "    'Ilid≈æa': 'Ilid≈æa',\n",
    "    'Butmir': 'Ilid≈æa',\n",
    "    'Sokoloviƒá Kolonija': 'Ilid≈æa',\n",
    "    'Otes': 'Ilid≈æa',\n",
    "    'Pejton': 'Ilid≈æa',\n",
    "    'Hrasnica': 'Ilid≈æa',\n",
    "    'Bla≈æuj': 'Ilid≈æa',\n",
    "    'Lu≈æani': 'Ilid≈æa',\n",
    "    'Luzani': 'Ilid≈æa',\n",
    "    'luzani': 'Ilid≈æa',\n",
    "    'Pijacna': 'Ilid≈æa',\n",
    "    'Stup': 'Ilid≈æa',\n",
    "    'Gray Residence': 'Ilid≈æa',\n",
    "    \n",
    "    \n",
    "    # Had≈æiƒái\n",
    "    'Had≈æiƒái': 'Had≈æiƒái',\n",
    "    'Pazariƒá': 'Had≈æiƒái',\n",
    "    'Tarƒçin': 'Had≈æiƒái',\n",
    "    'Tovi≈°': 'Had≈æiƒái',\n",
    "    \n",
    "    # Vogo≈°ƒáa\n",
    "    'Vogo≈°ƒáa': 'Vogo≈°ƒáa',\n",
    "    'Semizovac': 'Vogo≈°ƒáa',\n",
    "    'Kobilja Glava': 'Vogo≈°ƒáa',\n",
    "    'Vogoscanskih odreda': 'Vogo≈°ƒáa',\n",
    "    'Vogo≈°ƒáanskih odreda': 'Vogo≈°ƒáa',\n",
    "    'Hotonj': 'Vogo≈°ƒáa',\n",
    "    \n",
    "    # Ilija≈°\n",
    "    'Ilija≈°': 'Ilija≈°',\n",
    "    'Podlugovi': 'Ilija≈°',\n",
    "    \n",
    "    # Trnovo\n",
    "    'Trnovo': 'Trnovo',\n",
    "    'Trnovo - Bjela≈°nica': 'Trnovo',\n",
    "    'Bjela≈°nica': 'Trnovo',\n",
    "    'Artes Bjela≈°nica': 'Trnovo',\n",
    "    \n",
    "    # NOTE: Some locations are outside Sarajevo Canton:\n",
    "    # - Lukavica (East Sarajevo - not in Sarajevo Canton)\n",
    "    # - Jahorina (Pale municipality - not in Sarajevo Canton)\n",
    "    # - Ravna planina (part of Jahorina area)\n",
    "    # - Makarska (coastal city in Croatia)\n",
    "    # - Visoko (separate municipality, not in Sarajevo Canton)\n",
    "    # These will remain unmapped as they don't belong to the 9 target municipalities\n",
    "    \n",
    "    # Street names that can be mapped based on known locations:\n",
    "    'D≈æemala Bijediƒáa': 'Sarajevo - Novi Grad',\n",
    "    'Dr. Silve Rizvanbegoviƒá': 'Sarajevo - Centar',\n",
    "    'Dr.Silve Rizvanbegovic': 'Ilid≈æa',\n",
    "    'Silve Rizvanbegovic': 'Ilid≈æa',\n",
    "    'Josipa Slavenskog': 'Ilid≈æa',\n",
    "    'Mesa Selimovic': 'Sarajevo - Novi Grad',\n",
    "    'Svetozara ƒÜoroviƒáa': 'Sarajevo - Centar',\n",
    "    'Semira Fraste': 'Sarajevo - Novi Grad',\n",
    "    'F. Becirbegovica': 'Sarajevo - Novo Sarajevo',\n",
    "    'Tome Mendesa': 'Vogo≈°ƒáa',\n",
    "    'Ante Babiƒáa': 'Sarajevo - Novi Grad',\n",
    "    'Ibrahima Ljubovica': 'Ilid≈æa',\n",
    "    'Samira Catovica Kobre': 'Ilid≈æa',\n",
    "    'Ramiza Jasara': 'Ilid≈æa',\n",
    "    'ramiza jasara': 'Ilid≈æa',\n",
    "    'Karla Malya': 'Ilid≈æa',\n",
    "    'Trg solidarnosti': 'Sarajevo - Novi Grad',\n",
    "    'Nikole Sopa': 'Ilid≈æa',\n",
    "    'Skendera Kulenovica': 'Sarajevo - Stari Grad',\n",
    "    'Hifzi Bjelavca': 'Ilid≈æa',\n",
    "    'Be≈°areviƒáa': 'Sarajevo - Centar',\n",
    "    'Samin gaj': 'Ilid≈æa',\n",
    "    'Slatina': 'Sarajevo - Centar',\n",
    "    'Kod OHR-a': 'Sarajevo - Centar',\n",
    "    'Latiƒçka': 'Ilid≈æa',\n",
    "    'Stupska': 'Ilid≈æa',\n",
    "    'p.o.zvijezda': 'Vogo≈°ƒáa',\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Created mapping with {len(neighborhood_to_municipality)} neighborhood entries\")\n",
    "print(f\"   Mapping to {len(set(neighborhood_to_municipality.values()))} municipalities\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SAMPLE MAPPINGS:\")\n",
    "print(\"-\" * 80)\n",
    "for i, (neighborhood, municipality) in enumerate(list(neighborhood_to_municipality.items())[:10], 1):\n",
    "    print(f\"{i:3d}. {neighborhood:<30} ‚Üí {municipality}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check if title or municipality contains neighborhood keywords\n",
    "print(\"=\" * 80)\n",
    "print(\"SEARCHING FOR NEIGHBORHOODS IN MUNICIPALITY AND TITLE COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'municipality' in df.columns:\n",
    "    # Create a copy of the original municipality column for comparison\n",
    "    df['municipality_original'] = df['municipality'].copy()\n",
    "    \n",
    "    found_mappings = []\n",
    "    unmapped_records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        municipality_value = str(row['municipality']) if pd.notna(row['municipality']) else ''\n",
    "        title_value = str(row['title']) if pd.notna(row['title']) else ''\n",
    "        \n",
    "        # Combine both for searching\n",
    "        search_text = f\"{municipality_value} {title_value}\".lower()\n",
    "        \n",
    "        # Try to find a matching neighborhood\n",
    "        matched = False\n",
    "        for neighborhood, target_municipality in neighborhood_to_municipality.items():\n",
    "            if neighborhood.lower() in search_text:\n",
    "                # Found a match!\n",
    "                if pd.isna(row['municipality']) or row['municipality'] != target_municipality:\n",
    "                    found_mappings.append({\n",
    "                        'index': idx,\n",
    "                        'original_municipality': row['municipality'],\n",
    "                        'title': row['title'],\n",
    "                        'found_neighborhood': neighborhood,\n",
    "                        'mapped_to': target_municipality\n",
    "                    })\n",
    "                    # Update the municipality\n",
    "                    df.at[idx, 'municipality'] = target_municipality\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        # If no match found and municipality is not one of the target 9\n",
    "        target_municipalities = set(neighborhood_to_municipality.values())\n",
    "        if not matched and pd.notna(row['municipality']) and row['municipality'] not in target_municipalities:\n",
    "            unmapped_records.append({\n",
    "                'index': idx,\n",
    "                'municipality': row['municipality'],\n",
    "                'title': row['title']\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n‚úÖ FOUND {len(found_mappings)} records with neighborhood keywords\")\n",
    "    \n",
    "    if len(found_mappings) > 0:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"SAMPLE OF MAPPED RECORDS (first 10):\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, mapping in enumerate(found_mappings[:10], 1):\n",
    "            print(f\"\\n{i}. Found '{mapping['found_neighborhood']}' ‚Üí Mapped to '{mapping['mapped_to']}'\")\n",
    "            print(f\"   Original: {mapping['original_municipality']}\")\n",
    "            print(f\"   Title: {mapping['title'][:80]}...\")\n",
    "    \n",
    "    if len(unmapped_records) > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"‚ö†Ô∏è STILL HAVE {len(unmapped_records)} UNMAPPED RECORDS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nThese don't match any known neighborhood:\")\n",
    "        \n",
    "        # Show unique unmapped municipalities\n",
    "        unique_unmapped = {}\n",
    "        for record in unmapped_records:\n",
    "            mun = record['municipality']\n",
    "            if mun not in unique_unmapped:\n",
    "                unique_unmapped[mun] = []\n",
    "            unique_unmapped[mun].append(record['title'])\n",
    "        \n",
    "        for mun, titles in unique_unmapped.items():\n",
    "            print(f\"\\n  {mun} ({len(titles)} records)\")\n",
    "            print(f\"    Sample title: {titles[0][:80]}...\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All records successfully mapped!\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No 'municipality' column found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Show final municipality distribution after mapping\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MUNICIPALITY DISTRIBUTION AFTER MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'municipality' in df.columns:\n",
    "    print(\"\\nMunicipality Value Counts:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    final_counts = df['municipality'].value_counts()\n",
    "    for idx, (municipality, count) in enumerate(final_counts.items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{idx:3d}. {municipality:<35} {count:5d} records ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Check how many were successfully mapped\n",
    "    target_municipalities = [\n",
    "        'Had≈æiƒái', 'Ilid≈æa', 'Ilija≈°', 'Sarajevo - Centar',\n",
    "        'Sarajevo - Novi Grad', 'Sarajevo - Novo Sarajevo',\n",
    "        'Sarajevo - Stari Grad', 'Trnovo', 'Vogo≈°ƒáa'\n",
    "    ]\n",
    "    \n",
    "    mapped_count = df[df['municipality'].isin(target_municipalities)].shape[0]\n",
    "    unmapped_count = df[~df['municipality'].isin(target_municipalities) & df['municipality'].notna()].shape[0]\n",
    "    missing_count = df['municipality'].isnull().sum()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MAPPING SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚úÖ Mapped to target municipalities: {mapped_count} ({mapped_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"‚ö†Ô∏è  Still unmapped (non-standard):   {unmapped_count} ({unmapped_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"‚ùå Missing municipality data:       {missing_count} ({missing_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"\\nüìä Total records: {len(df)}\")\n",
    "    \n",
    "    # Show what changed\n",
    "    if 'municipality_original' in df.columns:\n",
    "        changed = df[df['municipality'] != df['municipality_original']].shape[0]\n",
    "        print(f\"\\nüîÑ Updated {changed} records with neighborhood mapping\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No 'municipality' column found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a054ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Remove records outside Sarajevo Canton\n",
    "print(\"=\" * 80)\n",
    "print(\"REMOVING RECORDS OUTSIDE SARAJEVO CANTON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define keywords for locations outside Sarajevo Canton\n",
    "outside_canton_keywords = [\n",
    "    'lukavica', 'lukavici', 'jahorina', 'jahoirina', 'ravna planina', \n",
    "    'makarska', 'visoko', 'dvori≈°ta', 'vuƒçko', 'pahulja', \n",
    "    'olovske luke', 'ajdinoviƒái', 'azapoviƒái', 'ponijeri', 'podvisoki', \n",
    "    'homolj', 'prhinje', 'bo≈°ka jugoviƒáa', 'dr. d≈æananoviƒáa', \n",
    "    'spasovdanska', 'srpskih vladara', 'bukova ravan', 'tu≈°njiƒái', \n",
    "    'be≈°agiƒáa visoko'\n",
    "]\n",
    "\n",
    "print(f\"\\nSearching for {len(outside_canton_keywords)} location keywords...\")\n",
    "print(f\"Keywords: {', '.join(outside_canton_keywords[:5])}... (and {len(outside_canton_keywords) - 5} more)\")\n",
    "\n",
    "# Track which records to remove\n",
    "records_to_remove = []\n",
    "keyword_counts = {keyword: [] for keyword in outside_canton_keywords}\n",
    "\n",
    "initial_count = len(df)\n",
    "\n",
    "# Search in both municipality and title columns (case-insensitive)\n",
    "for idx, row in df.iterrows():\n",
    "    municipality_text = str(row['municipality']).lower() if pd.notna(row['municipality']) else ''\n",
    "    title_text = str(row['title']).lower() if pd.notna(row['title']) else ''\n",
    "    search_text = f\"{municipality_text} {title_text}\"\n",
    "    \n",
    "    for keyword in outside_canton_keywords:\n",
    "        if keyword in search_text:\n",
    "            records_to_remove.append(idx)\n",
    "            keyword_counts[keyword].append({\n",
    "                'index': idx,\n",
    "                'municipality': row['municipality'],\n",
    "                'title': row['title'][:80]\n",
    "            })\n",
    "            break  # Only count each record once\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FOUND RECORDS TO REMOVE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show breakdown by keyword\n",
    "records_found = 0\n",
    "for keyword, records in keyword_counts.items():\n",
    "    if len(records) > 0:\n",
    "        records_found += len(records)\n",
    "        print(f\"\\nüîç Keyword '{keyword}': {len(records)} records\")\n",
    "        for i, record in enumerate(records[:3], 1):  # Show first 3 examples\n",
    "            print(f\"   {i}. {record['municipality']} - {record['title']}\")\n",
    "        if len(records) > 3:\n",
    "            print(f\"   ... and {len(records) - 3} more\")\n",
    "\n",
    "if records_found > 0:\n",
    "    # Remove the records\n",
    "    indices_to_drop = list(set(records_to_remove))  # Remove duplicates\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    final_count = len(df)\n",
    "    removed_count = initial_count - final_count\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"REMOVAL SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚úÖ Removed {removed_count} records outside Sarajevo Canton\")\n",
    "    print(f\"üìä Records before: {initial_count}\")\n",
    "    print(f\"üìä Records after:  {final_count}\")\n",
    "    print(f\"üìà Removed: {(removed_count/initial_count)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No records found outside Sarajevo Canton\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbacfb",
   "metadata": {},
   "source": [
    "## üìù Extract Municipalities from Descriptions\n",
    "\n",
    "For records with missing municipality data, let's search their descriptions for mentions of neighborhoods or municipalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06649c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract municipalities from descriptions for records with missing municipality data\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTRACTING MUNICIPALITIES FROM DESCRIPTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'municipality' in df.columns and 'description' in df.columns:\n",
    "    # Find records with missing municipality but have description\n",
    "    missing_municipality = df['municipality'].isnull()\n",
    "    has_description = df['description'].notna()\n",
    "    \n",
    "    records_to_check = missing_municipality & has_description\n",
    "    records_count = records_to_check.sum()\n",
    "    \n",
    "    print(f\"\\nüìä Records with missing municipality: {missing_municipality.sum()}\")\n",
    "    print(f\"üìä Records with description available: {has_description.sum()}\")\n",
    "    print(f\"üîç Records to check (missing municipality + has description): {records_count}\")\n",
    "    \n",
    "    if records_count > 0:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"SEARCHING DESCRIPTIONS FOR MUNICIPALITY KEYWORDS...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Use the same neighborhood mapping dictionary\n",
    "        found_from_description = []\n",
    "        \n",
    "        for idx, row in df[records_to_check].iterrows():\n",
    "            description_text = str(row['description']).lower() if pd.notna(row['description']) else ''\n",
    "            title_text = str(row['title']).lower() if pd.notna(row['title']) else ''\n",
    "            \n",
    "            # Combine description and title for better matching\n",
    "            search_text = f\"{description_text} {title_text}\"\n",
    "            \n",
    "            # Try to find a matching neighborhood in the description\n",
    "            matched = False\n",
    "            for neighborhood, target_municipality in neighborhood_to_municipality.items():\n",
    "                if neighborhood.lower() in search_text:\n",
    "                    # Found a match!\n",
    "                    found_from_description.append({\n",
    "                        'index': idx,\n",
    "                        'title': row['title'],\n",
    "                        'found_neighborhood': neighborhood,\n",
    "                        'mapped_to': target_municipality,\n",
    "                        'description_snippet': row['description'][:100] if pd.notna(row['description']) else ''\n",
    "                    })\n",
    "                    # Update the municipality\n",
    "                    df.at[idx, 'municipality'] = target_municipality\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        if len(found_from_description) > 0:\n",
    "            print(f\"\\n‚úÖ SUCCESS! Found {len(found_from_description)} municipalities from descriptions!\")\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "            print(\"SAMPLE OF EXTRACTED MUNICIPALITIES (first 10):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i, extraction in enumerate(found_from_description[:10], 1):\n",
    "                print(f\"\\n{i}. Found '{extraction['found_neighborhood']}' ‚Üí Mapped to '{extraction['mapped_to']}'\")\n",
    "                print(f\"   Title: {extraction['title'][:70]}...\")\n",
    "                print(f\"   Description: {extraction['description_snippet']}...\")\n",
    "            \n",
    "            if len(found_from_description) > 10:\n",
    "                print(f\"\\n   ... and {len(found_from_description) - 10} more\")\n",
    "            \n",
    "            # Show updated statistics\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"UPDATED MUNICIPALITY STATISTICS:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            target_municipalities = [\n",
    "                'Had≈æiƒái', 'Ilid≈æa', 'Ilija≈°', 'Sarajevo - Centar',\n",
    "                'Sarajevo - Novi Grad', 'Sarajevo - Novo Sarajevo',\n",
    "                'Sarajevo - Stari Grad', 'Trnovo', 'Vogo≈°ƒáa'\n",
    "            ]\n",
    "            \n",
    "            mapped_count = df[df['municipality'].isin(target_municipalities)].shape[0]\n",
    "            missing_count = df['municipality'].isnull().sum()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Mapped to target municipalities: {mapped_count} ({mapped_count/len(df)*100:.2f}%)\")\n",
    "            print(f\"‚ùå Still missing municipality data: {missing_count} ({missing_count/len(df)*100:.2f}%)\")\n",
    "            print(f\"üìà Improvement: +{len(found_from_description)} records mapped from descriptions\")\n",
    "            print(f\"\\nüìä Total records: {len(df)}\")\n",
    "            \n",
    "            # Show final distribution by municipality\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "            print(\"MUNICIPALITY DISTRIBUTION (after description extraction):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            final_counts = df['municipality'].value_counts()\n",
    "            for idx, (municipality, count) in enumerate(final_counts.items(), 1):\n",
    "                if municipality in target_municipalities:\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"{idx:3d}. {municipality:<35} {count:5d} records ({percentage:5.2f}%)\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No additional municipalities found in descriptions\")\n",
    "            print(\"    The descriptions may not contain explicit neighborhood/municipality names\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No records need description-based extraction\")\n",
    "        print(\"    All records either have municipality data or lack descriptions\")\n",
    "else:\n",
    "    print(\"‚ùå Required columns not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update the cleaned CSV file without property_type and ad_type columns\n",
    "# print(\"UPDATING CLEANED DATA FILE:\")\n",
    "# print(\"=\" * 80)\n",
    "# output_file = '../data/sarajevo_flats_nekretnine_cleaned_1.csv'\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"‚úÖ Updated cleaned data saved to: {output_file}\")\n",
    "# print(f\"Total columns: {len(df.columns)}\")\n",
    "# print(f\"Column names: {list(df.columns)}\")\n",
    "# print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML venv)",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
